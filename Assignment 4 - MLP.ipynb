{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noted-template",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-jewel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polished-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X = df.drop('quality', axis=1).values\n",
    "y = df['quality'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-visit",
   "metadata": {},
   "source": [
    "# Creating a basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "severe-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-simple",
   "metadata": {},
   "source": [
    "# Making changes to basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sunrise-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train).squeeze()\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100f169d-409f-408a-af81-16d39f77f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shelger/miniconda3/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 35.5854, Val Loss: 36.0617\n",
      "Epoch [20/100], Loss: 35.4117, Val Loss: 35.8851\n",
      "Epoch [30/100], Loss: 35.2391, Val Loss: 35.7096\n",
      "Epoch [40/100], Loss: 35.0678, Val Loss: 35.5352\n",
      "Epoch [50/100], Loss: 34.8975, Val Loss: 35.3620\n",
      "Epoch [60/100], Loss: 34.7283, Val Loss: 35.1898\n",
      "Epoch [70/100], Loss: 34.5604, Val Loss: 35.0188\n",
      "Epoch [80/100], Loss: 34.3936, Val Loss: 34.8490\n",
      "Epoch [90/100], Loss: 34.2278, Val Loss: 34.6801\n",
      "Epoch [100/100], Loss: 34.0631, Val Loss: 34.5123\n",
      "Test MSE: 34.5570\n"
     ]
    }
   ],
   "source": [
    "# Original hyperparameters\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test MSE: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af12de38-56b9-4c82-b5ce-fe2ffc67cf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 31.7346, Val Loss: 31.9421\n",
      "Epoch [20/300], Loss: 30.0132, Val Loss: 30.2020\n",
      "Epoch [30/300], Loss: 28.3856, Val Loss: 28.5540\n",
      "Epoch [40/300], Loss: 26.8375, Val Loss: 26.9842\n",
      "Epoch [50/300], Loss: 25.3589, Val Loss: 25.4816\n",
      "Epoch [60/300], Loss: 23.9420, Val Loss: 24.0410\n",
      "Epoch [70/300], Loss: 22.5828, Val Loss: 22.6587\n",
      "Epoch [80/300], Loss: 21.2804, Val Loss: 21.3347\n",
      "Epoch [90/300], Loss: 20.0353, Val Loss: 20.0689\n",
      "Epoch [100/300], Loss: 18.8500, Val Loss: 18.8643\n",
      "Epoch [110/300], Loss: 17.7245, Val Loss: 17.7225\n",
      "Epoch [120/300], Loss: 16.6627, Val Loss: 16.6462\n",
      "Epoch [130/300], Loss: 15.6665, Val Loss: 15.6367\n",
      "Epoch [140/300], Loss: 14.7363, Val Loss: 14.6943\n",
      "Epoch [150/300], Loss: 13.8732, Val Loss: 13.8217\n",
      "Epoch [160/300], Loss: 13.0758, Val Loss: 13.0180\n",
      "Epoch [170/300], Loss: 12.3408, Val Loss: 12.2794\n",
      "Epoch [180/300], Loss: 11.6666, Val Loss: 11.6020\n",
      "Epoch [190/300], Loss: 11.0456, Val Loss: 10.9787\n",
      "Epoch [200/300], Loss: 10.4764, Val Loss: 10.4072\n",
      "Epoch [210/300], Loss: 9.9522, Val Loss: 9.8820\n",
      "Epoch [220/300], Loss: 9.4689, Val Loss: 9.3985\n",
      "Epoch [230/300], Loss: 9.0201, Val Loss: 8.9500\n",
      "Epoch [240/300], Loss: 8.5996, Val Loss: 8.5300\n",
      "Epoch [250/300], Loss: 8.2036, Val Loss: 8.1358\n",
      "Epoch [260/300], Loss: 7.8303, Val Loss: 7.7627\n",
      "Epoch [270/300], Loss: 7.4739, Val Loss: 7.4064\n",
      "Epoch [280/300], Loss: 7.1322, Val Loss: 7.0633\n",
      "Epoch [290/300], Loss: 6.8027, Val Loss: 6.7343\n",
      "Epoch [300/300], Loss: 6.4842, Val Loss: 6.4151\n",
      "Test MSE: 6.4197\n"
     ]
    }
   ],
   "source": [
    "# increase epochs and reduce learning rate\n",
    "model = MLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, 300)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test MSE: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0554174e-a1bd-49e8-b19f-19c0b1a7f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/600], Loss: 19.9152, Val Loss: 18.8316\n",
      "Epoch [20/600], Loss: 9.6243, Val Loss: 8.7739\n",
      "Epoch [30/600], Loss: 4.4197, Val Loss: 3.8817\n",
      "Epoch [40/600], Loss: 1.5210, Val Loss: 1.2454\n",
      "Epoch [50/600], Loss: 0.6936, Val Loss: 0.6133\n",
      "Epoch [60/600], Loss: 0.5394, Val Loss: 0.4947\n",
      "Epoch [70/600], Loss: 0.4841, Val Loss: 0.4511\n",
      "Epoch [80/600], Loss: 0.4587, Val Loss: 0.4316\n",
      "Epoch [90/600], Loss: 0.4458, Val Loss: 0.4225\n",
      "Epoch [100/600], Loss: 0.4387, Val Loss: 0.4180\n",
      "Epoch [110/600], Loss: 0.4346, Val Loss: 0.4157\n",
      "Epoch [120/600], Loss: 0.4320, Val Loss: 0.4145\n",
      "Epoch [130/600], Loss: 0.4301, Val Loss: 0.4138\n",
      "Epoch [140/600], Loss: 0.4287, Val Loss: 0.4134\n",
      "Epoch [150/600], Loss: 0.4276, Val Loss: 0.4131\n",
      "Epoch [160/600], Loss: 0.4266, Val Loss: 0.4129\n",
      "Epoch [170/600], Loss: 0.4257, Val Loss: 0.4127\n",
      "Epoch [180/600], Loss: 0.4249, Val Loss: 0.4124\n",
      "Epoch [190/600], Loss: 0.4242, Val Loss: 0.4121\n",
      "Epoch [200/600], Loss: 0.4236, Val Loss: 0.4119\n",
      "Epoch [210/600], Loss: 0.4230, Val Loss: 0.4116\n",
      "Epoch [220/600], Loss: 0.4224, Val Loss: 0.4114\n",
      "Epoch [230/600], Loss: 0.4219, Val Loss: 0.4111\n",
      "Epoch [240/600], Loss: 0.4214, Val Loss: 0.4109\n",
      "Epoch [250/600], Loss: 0.4210, Val Loss: 0.4106\n",
      "Epoch [260/600], Loss: 0.4206, Val Loss: 0.4104\n",
      "Epoch [270/600], Loss: 0.4202, Val Loss: 0.4102\n",
      "Epoch [280/600], Loss: 0.4198, Val Loss: 0.4100\n",
      "Epoch [290/600], Loss: 0.4194, Val Loss: 0.4097\n",
      "Epoch [300/600], Loss: 0.4190, Val Loss: 0.4095\n",
      "Epoch [310/600], Loss: 0.4187, Val Loss: 0.4093\n",
      "Epoch [320/600], Loss: 0.4183, Val Loss: 0.4091\n",
      "Epoch [330/600], Loss: 0.4180, Val Loss: 0.4089\n",
      "Epoch [340/600], Loss: 0.4177, Val Loss: 0.4087\n",
      "Epoch [350/600], Loss: 0.4174, Val Loss: 0.4085\n",
      "Epoch [360/600], Loss: 0.4171, Val Loss: 0.4083\n",
      "Epoch [370/600], Loss: 0.4168, Val Loss: 0.4081\n",
      "Epoch [380/600], Loss: 0.4166, Val Loss: 0.4080\n",
      "Epoch [390/600], Loss: 0.4164, Val Loss: 0.4078\n",
      "Epoch [400/600], Loss: 0.4161, Val Loss: 0.4077\n",
      "Epoch [410/600], Loss: 0.4159, Val Loss: 0.4075\n",
      "Epoch [420/600], Loss: 0.4157, Val Loss: 0.4074\n",
      "Epoch [430/600], Loss: 0.4155, Val Loss: 0.4072\n",
      "Epoch [440/600], Loss: 0.4153, Val Loss: 0.4071\n",
      "Epoch [450/600], Loss: 0.4151, Val Loss: 0.4070\n",
      "Epoch [460/600], Loss: 0.4149, Val Loss: 0.4068\n",
      "Epoch [470/600], Loss: 0.4147, Val Loss: 0.4067\n",
      "Epoch [480/600], Loss: 0.4146, Val Loss: 0.4066\n",
      "Epoch [490/600], Loss: 0.4144, Val Loss: 0.4064\n",
      "Epoch [500/600], Loss: 0.4142, Val Loss: 0.4063\n",
      "Epoch [510/600], Loss: 0.4140, Val Loss: 0.4062\n",
      "Epoch [520/600], Loss: 0.4138, Val Loss: 0.4060\n",
      "Epoch [530/600], Loss: 0.4137, Val Loss: 0.4059\n",
      "Epoch [540/600], Loss: 0.4135, Val Loss: 0.4058\n",
      "Epoch [550/600], Loss: 0.4133, Val Loss: 0.4056\n",
      "Epoch [560/600], Loss: 0.4131, Val Loss: 0.4055\n",
      "Epoch [570/600], Loss: 0.4129, Val Loss: 0.4054\n",
      "Epoch [580/600], Loss: 0.4127, Val Loss: 0.4052\n",
      "Epoch [590/600], Loss: 0.4126, Val Loss: 0.4051\n",
      "Epoch [600/600], Loss: 0.4124, Val Loss: 0.4049\n",
      "Test MSE: 0.4074\n"
     ]
    }
   ],
   "source": [
    "# keep increasing epoch and reducing learning rate\n",
    "# to check if overfitting happens\n",
    "model = MLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, 600)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test MSE: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68e7e96-14cd-4237-a247-10d7839a2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 21.2051, Val Loss: 20.5321\n",
      "Epoch [20/1000], Loss: 12.0663, Val Loss: 11.5530\n",
      "Epoch [30/1000], Loss: 6.5643, Val Loss: 6.2887\n",
      "Epoch [40/1000], Loss: 3.7720, Val Loss: 3.6058\n",
      "Epoch [50/1000], Loss: 1.5153, Val Loss: 1.3946\n",
      "Epoch [60/1000], Loss: 0.7006, Val Loss: 0.6911\n",
      "Epoch [70/1000], Loss: 0.5479, Val Loss: 0.5496\n",
      "Epoch [80/1000], Loss: 0.5019, Val Loss: 0.4997\n",
      "Epoch [90/1000], Loss: 0.4777, Val Loss: 0.4721\n",
      "Epoch [100/1000], Loss: 0.4622, Val Loss: 0.4544\n",
      "Epoch [110/1000], Loss: 0.4517, Val Loss: 0.4425\n",
      "Epoch [120/1000], Loss: 0.4444, Val Loss: 0.4342\n",
      "Epoch [130/1000], Loss: 0.4393, Val Loss: 0.4283\n",
      "Epoch [140/1000], Loss: 0.4357, Val Loss: 0.4240\n",
      "Epoch [150/1000], Loss: 0.4331, Val Loss: 0.4209\n",
      "Epoch [160/1000], Loss: 0.4312, Val Loss: 0.4186\n",
      "Epoch [170/1000], Loss: 0.4297, Val Loss: 0.4168\n",
      "Epoch [180/1000], Loss: 0.4287, Val Loss: 0.4154\n",
      "Epoch [190/1000], Loss: 0.4278, Val Loss: 0.4143\n",
      "Epoch [200/1000], Loss: 0.4272, Val Loss: 0.4135\n",
      "Epoch [210/1000], Loss: 0.4267, Val Loss: 0.4128\n",
      "Epoch [220/1000], Loss: 0.4263, Val Loss: 0.4122\n",
      "Epoch [230/1000], Loss: 0.4259, Val Loss: 0.4117\n",
      "Epoch [240/1000], Loss: 0.4256, Val Loss: 0.4113\n",
      "Epoch [250/1000], Loss: 0.4254, Val Loss: 0.4109\n",
      "Epoch [260/1000], Loss: 0.4251, Val Loss: 0.4106\n",
      "Epoch [270/1000], Loss: 0.4249, Val Loss: 0.4103\n",
      "Epoch [280/1000], Loss: 0.4247, Val Loss: 0.4101\n",
      "Epoch [290/1000], Loss: 0.4246, Val Loss: 0.4098\n",
      "Epoch [300/1000], Loss: 0.4244, Val Loss: 0.4096\n",
      "Epoch [310/1000], Loss: 0.4243, Val Loss: 0.4094\n",
      "Epoch [320/1000], Loss: 0.4241, Val Loss: 0.4093\n",
      "Epoch [330/1000], Loss: 0.4240, Val Loss: 0.4091\n",
      "Epoch [340/1000], Loss: 0.4238, Val Loss: 0.4089\n",
      "Epoch [350/1000], Loss: 0.4237, Val Loss: 0.4088\n",
      "Epoch [360/1000], Loss: 0.4236, Val Loss: 0.4086\n",
      "Epoch [370/1000], Loss: 0.4235, Val Loss: 0.4085\n",
      "Epoch [380/1000], Loss: 0.4234, Val Loss: 0.4084\n",
      "Epoch [390/1000], Loss: 0.4233, Val Loss: 0.4083\n",
      "Epoch [400/1000], Loss: 0.4232, Val Loss: 0.4082\n",
      "Epoch [410/1000], Loss: 0.4231, Val Loss: 0.4081\n",
      "Epoch [420/1000], Loss: 0.4230, Val Loss: 0.4080\n",
      "Epoch [430/1000], Loss: 0.4229, Val Loss: 0.4079\n",
      "Epoch [440/1000], Loss: 0.4228, Val Loss: 0.4078\n",
      "Epoch [450/1000], Loss: 0.4227, Val Loss: 0.4077\n",
      "Epoch [460/1000], Loss: 0.4227, Val Loss: 0.4076\n",
      "Epoch [470/1000], Loss: 0.4226, Val Loss: 0.4075\n",
      "Epoch [480/1000], Loss: 0.4225, Val Loss: 0.4074\n",
      "Epoch [490/1000], Loss: 0.4224, Val Loss: 0.4074\n",
      "Epoch [500/1000], Loss: 0.4224, Val Loss: 0.4073\n",
      "Epoch [510/1000], Loss: 0.4223, Val Loss: 0.4072\n",
      "Epoch [520/1000], Loss: 0.4222, Val Loss: 0.4072\n",
      "Epoch [530/1000], Loss: 0.4221, Val Loss: 0.4071\n",
      "Epoch [540/1000], Loss: 0.4221, Val Loss: 0.4070\n",
      "Epoch [550/1000], Loss: 0.4220, Val Loss: 0.4070\n",
      "Epoch [560/1000], Loss: 0.4220, Val Loss: 0.4069\n",
      "Epoch [570/1000], Loss: 0.4219, Val Loss: 0.4069\n",
      "Epoch [580/1000], Loss: 0.4218, Val Loss: 0.4068\n",
      "Epoch [590/1000], Loss: 0.4218, Val Loss: 0.4067\n",
      "Epoch [600/1000], Loss: 0.4217, Val Loss: 0.4067\n",
      "Epoch [610/1000], Loss: 0.4217, Val Loss: 0.4066\n",
      "Epoch [620/1000], Loss: 0.4216, Val Loss: 0.4066\n",
      "Epoch [630/1000], Loss: 0.4216, Val Loss: 0.4065\n",
      "Epoch [640/1000], Loss: 0.4215, Val Loss: 0.4065\n",
      "Epoch [650/1000], Loss: 0.4215, Val Loss: 0.4064\n",
      "Epoch [660/1000], Loss: 0.4214, Val Loss: 0.4064\n",
      "Epoch [670/1000], Loss: 0.4214, Val Loss: 0.4064\n",
      "Epoch [680/1000], Loss: 0.4213, Val Loss: 0.4063\n",
      "Epoch [690/1000], Loss: 0.4213, Val Loss: 0.4063\n",
      "Epoch [700/1000], Loss: 0.4212, Val Loss: 0.4062\n",
      "Epoch [710/1000], Loss: 0.4212, Val Loss: 0.4062\n",
      "Epoch [720/1000], Loss: 0.4211, Val Loss: 0.4061\n",
      "Epoch [730/1000], Loss: 0.4211, Val Loss: 0.4061\n",
      "Epoch [740/1000], Loss: 0.4211, Val Loss: 0.4061\n",
      "Epoch [750/1000], Loss: 0.4210, Val Loss: 0.4060\n",
      "Epoch [760/1000], Loss: 0.4210, Val Loss: 0.4060\n",
      "Epoch [770/1000], Loss: 0.4209, Val Loss: 0.4060\n",
      "Epoch [780/1000], Loss: 0.4209, Val Loss: 0.4059\n",
      "Epoch [790/1000], Loss: 0.4209, Val Loss: 0.4059\n",
      "Epoch [800/1000], Loss: 0.4208, Val Loss: 0.4059\n",
      "Epoch [810/1000], Loss: 0.4208, Val Loss: 0.4058\n",
      "Epoch [820/1000], Loss: 0.4208, Val Loss: 0.4058\n",
      "Epoch [830/1000], Loss: 0.4207, Val Loss: 0.4058\n",
      "Epoch [840/1000], Loss: 0.4207, Val Loss: 0.4057\n",
      "Epoch [850/1000], Loss: 0.4207, Val Loss: 0.4057\n",
      "Epoch [860/1000], Loss: 0.4206, Val Loss: 0.4057\n",
      "Epoch [870/1000], Loss: 0.4206, Val Loss: 0.4056\n",
      "Epoch [880/1000], Loss: 0.4206, Val Loss: 0.4056\n",
      "Epoch [890/1000], Loss: 0.4205, Val Loss: 0.4056\n",
      "Epoch [900/1000], Loss: 0.4205, Val Loss: 0.4056\n",
      "Epoch [910/1000], Loss: 0.4205, Val Loss: 0.4055\n",
      "Epoch [920/1000], Loss: 0.4204, Val Loss: 0.4055\n",
      "Epoch [930/1000], Loss: 0.4204, Val Loss: 0.4055\n",
      "Epoch [940/1000], Loss: 0.4204, Val Loss: 0.4054\n",
      "Epoch [950/1000], Loss: 0.4203, Val Loss: 0.4054\n",
      "Epoch [960/1000], Loss: 0.4203, Val Loss: 0.4054\n",
      "Epoch [970/1000], Loss: 0.4203, Val Loss: 0.4054\n",
      "Epoch [980/1000], Loss: 0.4203, Val Loss: 0.4053\n",
      "Epoch [990/1000], Loss: 0.4202, Val Loss: 0.4053\n",
      "Epoch [1000/1000], Loss: 0.4202, Val Loss: 0.4053\n",
      "Test MSE: 0.4163\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, 1000)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test MSE: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9783e453-37bd-4f6b-8421-19d679766b07",
   "metadata": {},
   "source": [
    "It seems, when we increase the epoch into 1000, overfitting happens.\n",
    "Since the dataset has only 1000+ rows, I don't think dropout is a good idea. So let's change the optimizer and loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-territory",
   "metadata": {},
   "source": [
    "# Optional: Implementing the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secret-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
